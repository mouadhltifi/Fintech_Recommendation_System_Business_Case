n_layers=4
hidden_dim=512
learning_rate=0.0005672366495771769
batch_size=64
dropout=0.14204094889136684
weight_decay=0.0006458270267459578
optimizer=adam
activation=selu
scheduler=step
hidden_dims=[512, 512, 512, 512]
dropout_rates=[0.14204094889136684, 0.14204094889136684, 0.14204094889136684, 0.14204094889136684, 0.14204094889136684]
