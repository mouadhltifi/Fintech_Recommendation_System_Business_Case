n_layers=5
hidden_dim=64
learning_rate=0.006351167027557287
batch_size=32
dropout=0.42734472220117764
weight_decay=9.775975941576401e-08
optimizer=adamw
activation=leaky_relu
scheduler=step
hidden_dims=[64, 64, 64, 64, 64]
dropout_rates=[0.42734472220117764, 0.42734472220117764, 0.42734472220117764, 0.42734472220117764, 0.42734472220117764, 0.42734472220117764]
