n_layers=4
hidden_dim=128
learning_rate=0.0023192049482629947
batch_size=256
dropout=0.4446912859493567
weight_decay=3.216089691779078e-07
optimizer=adamw
activation=relu
scheduler=none
hidden_dims=[128, 128, 128, 128]
dropout_rates=[0.4446912859493567, 0.4446912859493567, 0.4446912859493567, 0.4446912859493567, 0.4446912859493567]
