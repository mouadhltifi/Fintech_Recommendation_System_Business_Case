n_layers=4
hidden_dim=32
learning_rate=0.0016454832515934968
batch_size=64
dropout=0.2787386976525861
weight_decay=6.027731434544262e-05
optimizer=adamw
activation=relu
scheduler=cosine
hidden_dims=[32, 32, 32, 32]
dropout_rates=[0.2787386976525861, 0.2787386976525861, 0.2787386976525861, 0.2787386976525861, 0.2787386976525861]
