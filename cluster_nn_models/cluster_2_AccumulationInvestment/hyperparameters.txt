n_layers=3
hidden_dim=128
learning_rate=5.53555307183523e-05
batch_size=128
dropout=0.32032040000948664
weight_decay=4.985939997457626e-07
optimizer=adam
activation=relu
scheduler=none
hidden_dims=[128, 128, 128]
dropout_rates=[0.32032040000948664, 0.32032040000948664, 0.32032040000948664, 0.32032040000948664]
